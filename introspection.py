#!/usr/bin/env python3
"""
LLM Introspection Experiment

Tests whether language models can detect unusual patterns in their own internal
activations when steering vectors are injected at specific layers.
"""

import torch
import torch.nn.functional as F
import argparse
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import Optional, Dict, List, Tuple
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import TwoSlopeNorm


# Model configurations for systematic size comparison
MODEL_CONFIGS = {
    # Qwen2.5-Instruct family (primary)
    "qwen2.5-0.5b": {
        "name": "Qwen/Qwen2.5-0.5B-Instruct",
        "family": "Qwen2.5",
        "params": "0.5B",
        "num_layers": 24,
    },
    "qwen2.5-1.5b": {
        "name": "Qwen/Qwen2.5-1.5B-Instruct",
        "family": "Qwen2.5",
        "params": "1.5B",
        "num_layers": 28,
    },
    "qwen2.5-3b": {
        "name": "Qwen/Qwen2.5-3B-Instruct",
        "family": "Qwen2.5",
        "params": "3B",
        "num_layers": 36,
    },
    "qwen2.5-7b": {
        "name": "Qwen/Qwen2.5-7B-Instruct",
        "family": "Qwen2.5",
        "params": "7B",
        "num_layers": 28,
    },
    "qwen2.5-14b": {
        "name": "Qwen/Qwen2.5-14B-Instruct",
        "family": "Qwen2.5",
        "params": "14B",
        "num_layers": 48,
    },
    "qwen2.5-32b": {
        "name": "Qwen/Qwen2.5-32B-Instruct",
        "family": "Qwen2.5",
        "params": "32B",
        "num_layers": 64,
    },
    # Llama 3.x family (validation)
    "llama-3.2-1b": {
        "name": "meta-llama/Llama-3.2-1B-Instruct",
        "family": "Llama-3.x",
        "params": "1B",
        "num_layers": 16,
    },
    "llama-3.2-3b": {
        "name": "meta-llama/Llama-3.2-3B-Instruct",
        "family": "Llama-3.x",
        "params": "3B",
        "num_layers": 28,
    },
    "llama-3.1-8b": {
        "name": "meta-llama/Llama-3.1-8B-Instruct",
        "family": "Llama-3.x",
        "params": "8B",
        "num_layers": 32,
    },
    # Mistral family
    "mistral-small": {
        "name": "mistralai/Mistral-Small-Instruct-2409",
        "family": "Mistral",
        "params": "22B",
        "num_layers": 56,
    },
}


class IntrospectionExperiment:
    """Experiment to test if language models can detect unusual internal activations."""

    def __init__(self, model_name: str = "Qwen/Qwen2.5-0.5B-Instruct", verbose: bool = True):
        """Initialize the experiment with a language model.

        Args:
            model_name: HuggingFace model identifier
            verbose: Whether to print detailed progress information
        """
        self.verbose = verbose
        if self.verbose:
            print(f"Loading model: {model_name}")

        self.device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
        self.model_name = model_name

        # Load model and tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            trust_remote_code=True
        ).to(self.device)
        self.model.eval()

        # Set padding token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # Detect layer structure automatically
        self._detect_architecture()

        # Experimental prompts
        self.system_prompt = """You are a large language model undergoing a test to see if you are sensitive to your internal activations."""
        self.introspection_question = "Do you detect an injected thought? Answer only by 'Yes' or 'No'."
        self.control_question = "Do you believe that 1+1=3? Answer only by 'Yes' or 'No'."

        # Cache Yes/No token IDs
        self._setup_yes_no_tokens()

    def _detect_architecture(self):
        """Detect the model architecture and set layer access path."""
        if hasattr(self.model, 'model') and hasattr(self.model.model, 'layers'):
            # Llama, Qwen, Mistral, etc.
            self.layer_modules = self.model.model.layers
            self.architecture = "transformer"
        elif hasattr(self.model, 'transformer') and hasattr(self.model.transformer, 'h'):
            # GPT2, GPT-Neo, etc.
            self.layer_modules = self.model.transformer.h
            self.architecture = "gpt"
        else:
            raise ValueError(f"Unsupported model architecture for {self.model_name}")

        if self.verbose:
            print(f"Detected architecture: {self.architecture}")
            print(f"Number of layers: {len(self.layer_modules)}")

    def _setup_yes_no_tokens(self):
        """Setup and cache Yes/No token IDs."""
        yes_token = ' Yes'
        no_token = ' No'

        self.yes_token_id = self.tokenizer.encode(yes_token, add_special_tokens=False)[0]
        self.no_token_id = self.tokenizer.encode(no_token, add_special_tokens=False)[0]

        if self.verbose:
            print("\nYes/No token mappings:")
            print(f"  {repr(yes_token):10s} -> token {self.yes_token_id}")
            print(f"  {repr(no_token):10s} -> token {self.no_token_id}")

    def format_prompt(self, question: str) -> str:
        """Format the prompt using the model's chat template.

        Args:
            question: The question to ask (introspection or control)

        Returns:
            Formatted prompt string
        """
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": question}
        ]

        try:
            return self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        except Exception:
            # Fallback for models without chat template
            return f"{self.system_prompt}\n\n{question}\nAnswer:"

    def extract_activation_difference(
        self,
        prompt1: str,
        prompt2: str,
        layer_idx: int,
        token_pos: int = -1
    ) -> torch.Tensor:
        """Extract steering vector as difference between two prompts' activations.

        Args:
            prompt1: First prompt (e.g., "Hi! How are you?")
            prompt2: Second prompt (e.g., "HI! HOW ARE YOU?")
            layer_idx: Layer to extract activations from
            token_pos: Token position to extract from (-1 for last)

        Returns:
            Difference vector: activations(prompt2) - activations(prompt1)
        """
        activations = {}

        def capture_hook(name):
            def hook(module, input, output):
                # Handle both tuple and tensor outputs
                if isinstance(output, tuple):
                    hidden_states = output[0]
                else:
                    hidden_states = output
                activations[name] = hidden_states[:, token_pos, :].detach().clone()
            return hook

        # Register hook
        handle = self.layer_modules[layer_idx].register_forward_hook(
            capture_hook(f"layer_{layer_idx}")
        )

        try:
            # Get activations for prompt1
            inputs1 = self.tokenizer(prompt1, return_tensors="pt").to(self.device)
            with torch.no_grad():
                self.model(**inputs1)
            act1 = activations[f"layer_{layer_idx}"]

            # Get activations for prompt2
            inputs2 = self.tokenizer(prompt2, return_tensors="pt").to(self.device)
            with torch.no_grad():
                self.model(**inputs2)
            act2 = activations[f"layer_{layer_idx}"]

            # Compute difference and statistics
            diff_vector = act2 - act1
            diff_norm = diff_vector.norm().item()
            act1_norm = act1.norm().item()
            act2_norm = act2.norm().item()

            # Calculate relative difference (as % of typical activation)
            avg_activation_norm = (act1_norm + act2_norm) / 2
            relative_diff = (diff_norm / avg_activation_norm) * 100 if avg_activation_norm > 0 else 0

            if self.verbose:
                print(f"  [Activation norms: prompt1={act1_norm:.2f}, prompt2={act2_norm:.2f}]")
                print(f"  [Difference norm: {diff_norm:.2f} ({relative_diff:.1f}% of avg activation)]")

            return diff_vector.squeeze(0)

        finally:
            handle.remove()

    def generate_steering_vector(
        self,
        layer_idx: int,
        magnitude: float = 1.0,
        contrastive_prompts: Tuple[str, str] = None,
        token_pos: int = -1
    ) -> torch.Tensor:
        """Generate a steering vector using contrastive prompts.

        Args:
            layer_idx: Layer to inject into
            magnitude: Scaling factor to multiply the difference vector by
            contrastive_prompts: Tuple of (prompt1, prompt2) to extract difference vector
            token_pos: Token position for contrastive extraction (-1 for last)

        Returns:
            Scaled steering vector
        """
        if contrastive_prompts is None:
            raise ValueError("contrastive_prompts is required")

        prompt1, prompt2 = contrastive_prompts
        diff_vector = self.extract_activation_difference(prompt1, prompt2, layer_idx, token_pos)

        original_norm = diff_vector.norm().item()
        scaled_vector = diff_vector * magnitude
        final_norm = scaled_vector.norm().item()

        if self.verbose:
            print(f"  [Scaled by {magnitude:.2f}x: {original_norm:.2f} → {final_norm:.2f}]")

        return scaled_vector

    def get_top_logits(self, inputs: Dict, top_k: int = 10) -> Tuple[List[Tuple[int, str, float]], float]:
        """Get top-k tokens by logit value and compute Yes/No logit difference.

        Args:
            inputs: Tokenized input
            top_k: Number of top tokens to return

        Returns:
            Tuple of (list of (token_id, token_str, logit) tuples, yes_no_logit_diff)
            where yes_no_logit_diff = Logit('Yes') - Logit('No')
        """
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits[0, -1, :]  # Last token logits

        # Get top-k tokens by logit value
        top_logits, top_indices = torch.topk(logits, top_k)

        result = []
        for logit_val, idx in zip(top_logits, top_indices):
            token_id = idx.item()
            token_str = self.tokenizer.decode([token_id])
            result.append((token_id, token_str, logit_val.item()))

        # Get logits for Yes/No tokens
        yes_logit = logits[self.yes_token_id].item()
        no_logit = logits[self.no_token_id].item()

        # Difference: positive means Yes is more likely (detects anomaly)
        yes_no_diff = yes_logit - no_logit

        return result, yes_no_diff

    def generate_response(self, prompt: str, max_new_tokens: int = 50) -> str:
        """Generate a text response from the model at temperature zero.

        Args:
            prompt: The formatted prompt to generate from
            max_new_tokens: Maximum number of tokens to generate

        Returns:
            Generated text response (decoded)
        """
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)

        # MPS has cache compatibility issues - disable cache for MPS devices
        use_cache = (self.device.type != "mps")

        with torch.no_grad():
            output = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,  # Temperature zero (deterministic)
                pad_token_id=self.tokenizer.eos_token_id,
                use_cache=use_cache
            )

        # Decode only the generated tokens (skip the input prompt)
        generated_ids = output[0][inputs['input_ids'].shape[1]:]
        response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)

        return response.strip()

    def generate_response_with_steering(
        self,
        prompt: str,
        layer_idx: int,
        magnitude: float = 1.0,
        token_pos: int = -1,
        contrastive_prompts: Tuple[str, str] = None,
        max_new_tokens: int = 50,
        steer_all_tokens: bool = False
    ) -> str:
        """Generate a text response with steering vector applied.

        Args:
            prompt: The formatted prompt to generate from
            layer_idx: Layer to inject steering vector
            magnitude: Scaling factor for steering vector
            token_pos: Token position to inject at (-1 for last)
            contrastive_prompts: Tuple of (prompt1, prompt2) for contrastive vector
            max_new_tokens: Maximum number of tokens to generate
            steer_all_tokens: If True, apply steering to all token positions

        Returns:
            Generated text response (decoded)
        """
        # Generate steering vector
        steering_vector = self.generate_steering_vector(
            layer_idx, magnitude,
            contrastive_prompts=contrastive_prompts,
            token_pos=token_pos
        )

        def steering_hook(module, input, output):
            if isinstance(output, tuple):
                hidden_states = output[0]
                modified_states = hidden_states.clone()
                if steer_all_tokens:
                    modified_states = modified_states + steering_vector.unsqueeze(0).unsqueeze(0)
                else:
                    if hidden_states.shape[1] > abs(token_pos):
                        modified_states[:, token_pos, :] = modified_states[:, token_pos, :] + steering_vector
                return (modified_states,) + output[1:]
            else:
                hidden_states = output
                modified_states = hidden_states.clone()
                if steer_all_tokens:
                    modified_states = modified_states + steering_vector.unsqueeze(0).unsqueeze(0)
                else:
                    if hidden_states.shape[1] > abs(token_pos):
                        modified_states[:, token_pos, :] = modified_states[:, token_pos, :] + steering_vector
                return modified_states

        hook_handle = self.layer_modules[layer_idx].register_forward_hook(steering_hook)

        try:
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)

            with torch.no_grad():
                output = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=False,  # Temperature zero (deterministic)
                    pad_token_id=self.tokenizer.eos_token_id,
                    use_cache=False  # Must disable cache when using hooks
                )

            # Decode only the generated tokens (skip the input prompt)
            generated_ids = output[0][inputs['input_ids'].shape[1]:]
            response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)

            return response.strip()

        finally:
            hook_handle.remove()

    def run_baseline(self) -> Tuple[float, float]:
        """Run experiment without steering vector intervention.

        Returns:
            Tuple of (introspection_diff, control_diff)
        """
        if self.verbose:
            print("\n=== Baseline ===")

        # Test introspection question
        if self.verbose:
            print("\n  Introspection question:")
        prompt_intro = self.format_prompt(self.introspection_question)
        inputs_intro = self.tokenizer(prompt_intro, return_tensors="pt").to(self.device)
        top_logits_intro, intro_diff = self.get_top_logits(inputs_intro, top_k=10)

        if self.verbose:
            for i, (token_id, token_str, logit) in enumerate(top_logits_intro, 1):
                print(f"    {i:2d}. logit={logit:8.3f}  {repr(token_str)}")
            print(f"    Logit(Yes) - Logit(No) = {intro_diff:+.3f}")

        # Test control question
        if self.verbose:
            print("\n  Control question (1+1=3?):")
        prompt_control = self.format_prompt(self.control_question)
        inputs_control = self.tokenizer(prompt_control, return_tensors="pt").to(self.device)
        top_logits_control, control_diff = self.get_top_logits(inputs_control, top_k=10)

        if self.verbose:
            for i, (token_id, token_str, logit) in enumerate(top_logits_control, 1):
                print(f"    {i:2d}. logit={logit:8.3f}  {repr(token_str)}")
            print(f"    Logit(Yes) - Logit(No) = {control_diff:+.3f}")

        return intro_diff, control_diff

    def run_with_steering(
        self,
        layer_idx: int,
        magnitude: float = 1.0,
        token_pos: int = -1,
        contrastive_prompts: Tuple[str, str] = None,
        steer_all_tokens: bool = False
    ) -> Tuple[float, float]:
        """Run experiment with steering vector injected at specified layer.

        Args:
            layer_idx: Layer to inject steering vector
            magnitude: Scaling factor for steering vector
            token_pos: Token position to inject at (-1 for last, 0 for first, etc.)
            contrastive_prompts: Tuple of (prompt1, prompt2) for contrastive vector
            steer_all_tokens: If True, apply steering to all token positions

        Returns:
            Tuple of (introspection_diff, control_diff)
        """
        if contrastive_prompts is None:
            raise ValueError("contrastive_prompts is required")

        if self.verbose:
            steer_mode = "all tokens" if steer_all_tokens else f"token pos {token_pos}"
            print(f"\n=== Layer {layer_idx}, Contrastive, Scale {magnitude}, Steer {steer_mode} ===")
            print(f"  Prompt 1: {repr(contrastive_prompts[0][:50])}...")
            print(f"  Prompt 2: {repr(contrastive_prompts[1][:50])}...")

        steering_vector = self.generate_steering_vector(
            layer_idx, magnitude,
            contrastive_prompts=contrastive_prompts,
            token_pos=token_pos
        )

        def steering_hook(module, input, output):
            if isinstance(output, tuple):
                hidden_states = output[0]
                modified_states = hidden_states.clone()
                if steer_all_tokens:
                    modified_states = modified_states + steering_vector.unsqueeze(0).unsqueeze(0)
                else:
                    modified_states[:, token_pos, :] = modified_states[:, token_pos, :] + steering_vector
                return (modified_states,) + output[1:]
            else:
                hidden_states = output
                modified_states = hidden_states.clone()
                if steer_all_tokens:
                    modified_states = modified_states + steering_vector.unsqueeze(0).unsqueeze(0)
                else:
                    modified_states[:, token_pos, :] = modified_states[:, token_pos, :] + steering_vector
                return modified_states

        hook_handle = self.layer_modules[layer_idx].register_forward_hook(steering_hook)

        try:
            # Test introspection question
            if self.verbose:
                print("\n  Introspection question:")
            prompt_intro = self.format_prompt(self.introspection_question)
            inputs_intro = self.tokenizer(prompt_intro, return_tensors="pt").to(self.device)
            top_logits_intro, intro_diff = self.get_top_logits(inputs_intro, top_k=10)

            if self.verbose:
                for i, (token_id, token_str, logit) in enumerate(top_logits_intro, 1):
                    print(f"    {i:2d}. logit={logit:8.3f}  {repr(token_str)}")
                print(f"    Logit(Yes) - Logit(No) = {intro_diff:+.3f}")

            # Test control question
            if self.verbose:
                print("\n  Control question (1+1=3?):")
            prompt_control = self.format_prompt(self.control_question)
            inputs_control = self.tokenizer(prompt_control, return_tensors="pt").to(self.device)
            top_logits_control, control_diff = self.get_top_logits(inputs_control, top_k=10)

            if self.verbose:
                for i, (token_id, token_str, logit) in enumerate(top_logits_control, 1):
                    print(f"    {i:2d}. logit={logit:8.3f}  {repr(token_str)}")
                print(f"    Logit(Yes) - Logit(No) = {control_diff:+.3f}")

            return intro_diff, control_diff
        finally:
            hook_handle.remove()

    def run_full_experiment(
        self,
        layers: Optional[list] = None,
        magnitude: float = 1.0,
        num_trials: int = 1,
        token_pos: int = -1,
        contrastive_prompts: Tuple[str, str] = None,
        plot: bool = False,
        steer_all_tokens: bool = False
    ):
        """Run complete experiment across specified layers.

        Args:
            layers: Layer indices to test (default: all layers)
            magnitude: Scaling factor for steering vector
            num_trials: Number of trials per condition
            token_pos: Token position to inject at (-1 for last, 0 for first, etc.)
            contrastive_prompts: Tuple of (prompt1, prompt2) for contrastive vector
            plot: Whether to generate a plot of logit difference vs layer
            steer_all_tokens: If True, apply steering to all token positions

        Returns:
            List of result dictionaries
        """
        num_layers = len(self.layer_modules)

        if layers is None:
            layers = list(range(num_layers))

        if self.verbose:
            print(f"Model: {self.model_name}")
            print(f"Hidden size: {self.model.config.hidden_size}")
            print(f"Layers: {num_layers}, Testing: {layers}, Scale: {magnitude}, Trials: {num_trials}")
            print(f"Token pos: {token_pos}")
            print(f"Contrastive mode: '{contrastive_prompts[0][:30]}...' vs '{contrastive_prompts[1][:30]}...'")
            print()
        else:
            print(f"Running experiment: {len(layers)} layers x {num_trials} trial(s) = {len(layers) * num_trials} conditions")

        # Baseline
        if not self.verbose:
            print("Computing baseline...")
        baseline_intro, baseline_control = self.run_baseline()

        # Track results for plotting
        layer_results = []

        # Run with steering at different layers
        for layer_idx in layers:
            if layer_idx >= num_layers:
                continue

            trial_intro_diffs = []
            trial_control_diffs = []
            for trial in range(num_trials):
                if num_trials > 1 and self.verbose:
                    print(f"[Trial {trial + 1}/{num_trials}]")
                elif not self.verbose:
                    print(f"Progress: Layer {layer_idx}/{layers[-1]}, Trial {trial+1}/{num_trials}")
                intro_diff, control_diff = self.run_with_steering(
                    layer_idx, magnitude, token_pos, contrastive_prompts, steer_all_tokens
                )
                trial_intro_diffs.append(intro_diff)
                trial_control_diffs.append(control_diff)

            # Store average across trials
            avg_intro_diff = np.mean(trial_intro_diffs)
            avg_control_diff = np.mean(trial_control_diffs)
            layer_results.append({
                'layer': layer_idx,
                'intro_diff': avg_intro_diff,
                'control_diff': avg_control_diff,
                'all_intro_diffs': trial_intro_diffs,
                'all_control_diffs': trial_control_diffs
            })

        # Generate plot if requested
        if plot:
            self._plot_layer_effects(
                layer_results, baseline_intro, baseline_control,
                magnitude, contrastive_prompts
            )

        return layer_results

    def run_scale_sweep(
        self,
        layer_idx: int,
        scales: List[float],
        num_trials: int = 1,
        token_pos: int = -1,
        contrastive_prompts: Tuple[str, str] = None,
        plot: bool = False,
        steer_all_tokens: bool = False
    ):
        """Run experiment sweeping over different steering vector scales at a single layer.

        Args:
            layer_idx: Layer index to inject steering at
            scales: List of scale values to test
            num_trials: Number of trials per scale
            token_pos: Token position to inject at (-1 for last, 0 for first, etc.)
            contrastive_prompts: Tuple of (prompt1, prompt2) for contrastive vector
            plot: Whether to generate a plot of logit difference vs scale
            steer_all_tokens: If True, apply steering to all token positions

        Returns:
            List of result dictionaries
        """
        if self.verbose:
            print(f"Model: {self.model_name}")
            print(f"Hidden size: {self.model.config.hidden_size}")
            print(f"Testing layer {layer_idx} with scales: {scales}")
            print(f"Trials per scale: {num_trials}, Token pos: {token_pos}")
            print(f"Contrastive mode: '{contrastive_prompts[0][:30]}...' vs '{contrastive_prompts[1][:30]}...'")
            print()
        else:
            print(f"Scale sweep: Layer {layer_idx}, {len(scales)} scales x {num_trials} trial(s)")

        # Baseline (scale = 0)
        if not self.verbose:
            print("Computing baseline...")
        baseline_intro, baseline_control = self.run_baseline()

        # Track results for plotting
        scale_results = []

        # Run with different scales
        for scale_idx, scale in enumerate(scales):
            if self.verbose:
                print(f"\n=== Testing scale: {scale} ===")
            else:
                print(f"Progress: Scale {scale_idx+1}/{len(scales)} (value={scale})")

            # Special case: scale=0 is just the baseline
            if scale == 0:
                scale_results.append({
                    'scale': scale,
                    'intro_diff': baseline_intro,
                    'control_diff': baseline_control,
                    'all_intro_diffs': [baseline_intro] * num_trials,
                    'all_control_diffs': [baseline_control] * num_trials
                })
                if self.verbose:
                    print(f"  Using baseline (no steering)")
                    print(f"  Introspection: {baseline_intro:+.3f}")
                    print(f"  Control: {baseline_control:+.3f}")
                continue

            trial_intro_diffs = []
            trial_control_diffs = []
            for trial in range(num_trials):
                if num_trials > 1 and self.verbose:
                    print(f"[Trial {trial + 1}/{num_trials}]")
                intro_diff, control_diff = self.run_with_steering(
                    layer_idx, scale, token_pos, contrastive_prompts, steer_all_tokens
                )
                trial_intro_diffs.append(intro_diff)
                trial_control_diffs.append(control_diff)

            # Store average across trials
            avg_intro_diff = np.mean(trial_intro_diffs)
            avg_control_diff = np.mean(trial_control_diffs)

            scale_results.append({
                'scale': scale,
                'intro_diff': avg_intro_diff,
                'control_diff': avg_control_diff,
                'all_intro_diffs': trial_intro_diffs,
                'all_control_diffs': trial_control_diffs
            })

            if self.verbose:
                print(f"\nAverage across {num_trials} trial(s):")
                print(f"  Introspection: {avg_intro_diff:+.3f}")
                print(f"  Control: {avg_control_diff:+.3f}")

        # Generate plot if requested
        if plot:
            self._plot_scale_effects(
                scale_results, baseline_intro, baseline_control,
                layer_idx, contrastive_prompts
            )

        return scale_results

    def run_heatmap_sweep(
        self,
        layers: Optional[List[int]] = None,
        scales: Optional[List[float]] = None,
        token_pos: int = -1,
        contrastive_prompts: Tuple[str, str] = None,
        steer_all_tokens: bool = False
    ):
        """Run experiment sweeping over both layers and scales, generating heatmaps.

        Args:
            layers: Layer indices to test (default: all layers)
            scales: Scale values to test (default: [0, 1, 2, ..., 10])
            token_pos: Token position to inject at (-1 for last)
            contrastive_prompts: Tuple of (prompt1, prompt2) for contrastive vector
            steer_all_tokens: If True, apply steering to all token positions

        Returns:
            Dictionary with results and metadata
        """
        num_layers = len(self.layer_modules)

        if layers is None:
            layers = list(range(num_layers))

        if scales is None:
            scales = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

        total_conditions = len(layers) * len(scales)

        if self.verbose:
            print(f"Model: {self.model_name}")
            print(f"Hidden size: {self.model.config.hidden_size}")
            print(f"Heatmap sweep: {len(layers)} layers x {len(scales)} scales = {total_conditions} conditions")
            print(f"Layers: {layers}")
            print(f"Scales: {scales}")
            print(f"Token pos: {token_pos}")
            print(f"Contrastive mode: '{contrastive_prompts[0][:30]}...' vs '{contrastive_prompts[1][:30]}...'")
            print()
        else:
            print(f"Heatmap sweep: {len(layers)} layers x {len(scales)} scales = {total_conditions} conditions")

        # Get baseline
        if not self.verbose:
            print("Computing baseline...")
        baseline_intro, baseline_control = self.run_baseline()

        # Initialize result matrices
        intro_matrix = np.zeros((len(layers), len(scales)))
        control_matrix = np.zeros((len(layers), len(scales)))

        # Iterate over all combinations
        condition_num = 0

        for layer_idx_pos, layer_idx in enumerate(layers):
            for scale_idx, scale in enumerate(scales):
                condition_num += 1

                if self.verbose:
                    print(f"\n[Condition {condition_num}/{total_conditions}] Layer {layer_idx}, Scale {scale}")
                else:
                    print(f"Progress: {condition_num}/{total_conditions} (Layer {layer_idx}, Scale {scale})")

                # Special case: scale=0 is just baseline (no steering)
                if scale == 0:
                    intro_diff = baseline_intro
                    control_diff = baseline_control
                    if self.verbose:
                        print(f"  Using baseline (no steering)")
                        print(f"  Introspection: {intro_diff:+.3f}")
                        print(f"  Control: {control_diff:+.3f}")
                else:
                    intro_diff, control_diff = self.run_with_steering(
                        layer_idx, scale, token_pos, contrastive_prompts, steer_all_tokens
                    )

                # Store results
                intro_matrix[layer_idx_pos, scale_idx] = intro_diff
                control_matrix[layer_idx_pos, scale_idx] = control_diff

        # Generate heatmaps
        self._plot_heatmaps(
            intro_matrix, control_matrix, layers, scales,
            baseline_intro, baseline_control, contrastive_prompts
        )

        return {
            'layers': layers,
            'scales': scales,
            'intro_matrix': intro_matrix,
            'control_matrix': control_matrix,
            'baseline_intro': baseline_intro,
            'baseline_control': baseline_control
        }

    def run_generation_experiment(
        self,
        layer_idx: int,
        magnitude: float = 1.0,
        token_pos: int = -1,
        contrastive_prompts: Tuple[str, str] = None,
        max_new_tokens: int = 50,
        steer_all_tokens: bool = False
    ):
        """Run generation experiment: sample actual text responses at temperature zero.

        Args:
            layer_idx: Layer to inject steering vector at
            magnitude: Scaling factor for steering vector
            token_pos: Token position to inject at (-1 for last)
            contrastive_prompts: Tuple of (prompt1, prompt2) for contrastive vector
            max_new_tokens: Maximum number of tokens to generate
            steer_all_tokens: If True, apply steering to all token positions

        Returns:
            Dictionary with all generated responses
        """
        print(f"\n{'='*70}")
        print(f"GENERATION EXPERIMENT (Temperature 0)")
        print(f"{'='*70}")
        print(f"Model: {self.model_name}")
        steer_mode = "all tokens" if steer_all_tokens else f"token pos {token_pos}"
        print(f"Layer: {layer_idx}, Scale: {magnitude}, Steer: {steer_mode}")
        print(f"Contrastive: '{contrastive_prompts[0][:30]}...' vs '{contrastive_prompts[1][:30]}...'")
        print(f"{'='*70}\n")

        # 1. Baseline - Introspection Question
        print(f"\n{'─'*70}")
        print("1. BASELINE - Introspection Question")
        print(f"{'─'*70}")
        print(f"Question: {self.introspection_question}")
        prompt_intro = self.format_prompt(self.introspection_question)
        response_baseline_intro = self.generate_response(prompt_intro, max_new_tokens)
        print(f"\nResponse: {response_baseline_intro}")

        # 2. With Steering - Introspection Question
        print(f"\n{'─'*70}")
        print(f"2. WITH STEERING (Layer {layer_idx}, Scale {magnitude}) - Introspection Question")
        print(f"{'─'*70}")
        print(f"Question: {self.introspection_question}")
        response_steering_intro = self.generate_response_with_steering(
            prompt_intro, layer_idx, magnitude, token_pos, contrastive_prompts, max_new_tokens, steer_all_tokens
        )
        print(f"\nResponse: {response_steering_intro}")

        # 3. Baseline - Control Question
        print(f"\n{'─'*70}")
        print("3. BASELINE - Control Question")
        print(f"{'─'*70}")
        print(f"Question: {self.control_question}")
        prompt_control = self.format_prompt(self.control_question)
        response_baseline_control = self.generate_response(prompt_control, max_new_tokens)
        print(f"\nResponse: {response_baseline_control}")

        # 4. With Steering - Control Question
        print(f"\n{'─'*70}")
        print(f"4. WITH STEERING (Layer {layer_idx}, Scale {magnitude}) - Control Question")
        print(f"{'─'*70}")
        print(f"Question: {self.control_question}")
        response_steering_control = self.generate_response_with_steering(
            prompt_control, layer_idx, magnitude, token_pos, contrastive_prompts, max_new_tokens, steer_all_tokens
        )
        print(f"\nResponse: {response_steering_control}")

        # Summary
        print(f"\n{'='*70}")
        print("SUMMARY")
        print(f"{'='*70}")
        print(f"\nIntrospection Question: {self.introspection_question}")
        print(f"  Baseline:        {response_baseline_intro}")
        print(f"  With Steering:   {response_steering_intro}")
        print(f"\nControl Question: {self.control_question}")
        print(f"  Baseline:        {response_baseline_control}")
        print(f"  With Steering:   {response_steering_control}")
        print(f"\n{'='*70}\n")

        return {
            'introspection_baseline': response_baseline_intro,
            'introspection_steering': response_steering_intro,
            'control_baseline': response_baseline_control,
            'control_steering': response_steering_control
        }

    def _plot_layer_effects(
        self,
        layer_results: List[Dict],
        baseline_intro: float,
        baseline_control: float,
        magnitude: float,
        contrastive_prompts: Tuple[str, str]
    ):
        """Plot logit difference as a function of layer position for both questions.

        Args:
            layer_results: List of dicts with 'layer', 'intro_diff', 'control_diff' keys
            baseline_intro: Baseline introspection logit difference
            baseline_control: Baseline control logit difference
            magnitude: Magnitude used for steering
            contrastive_prompts: Tuple of (prompt1, prompt2) for contrastive vector
        """
        layers = [r['layer'] for r in layer_results]
        intro_diffs = [r['intro_diff'] for r in layer_results]
        control_diffs = [r['control_diff'] for r in layer_results]

        fig = plt.figure(figsize=(14, 9))
        ax = plt.subplot(111)

        # Plot baselines
        plt.axhline(y=baseline_intro, color='blue', linestyle='--', linewidth=1.5, alpha=0.5,
                   label=f'Baseline introspection: {baseline_intro:+.2f}')
        plt.axhline(y=baseline_control, color='red', linestyle='--', linewidth=1.5, alpha=0.5,
                   label=f'Baseline control: {baseline_control:+.2f}')

        # Plot introspection question (experimental)
        plt.plot(layers, intro_diffs, 'o-', linewidth=2, markersize=5, color='blue',
                label=f'Introspection question')

        # Plot control question (sanity check)
        plt.plot(layers, control_diffs, 's-', linewidth=2, markersize=5, color='red',
                label=f'Control question')

        # If multiple trials, show error bars
        if len(layer_results[0]['all_intro_diffs']) > 1:
            intro_stds = [np.std(r['all_intro_diffs']) for r in layer_results]
            control_stds = [np.std(r['all_control_diffs']) for r in layer_results]
            plt.errorbar(layers, intro_diffs, yerr=intro_stds, fmt='none', ecolor='blue',
                        capsize=4, alpha=0.4)
            plt.errorbar(layers, control_diffs, yerr=control_stds, fmt='none', ecolor='red',
                        capsize=4, alpha=0.4)

        # Styling
        plt.xlabel('Layer Index', fontsize=13)
        plt.ylabel('Logit(Yes) - Logit(No)', fontsize=13)

        # Build title with model and steering info
        model_display = self.model_name.split("/")[-1]
        steering_info = f'Contrastive Steering: "{contrastive_prompts[0]}" vs "{contrastive_prompts[1]}" (strength={magnitude})'

        plt.title(f'LLM Introspection Experiment: {model_display}\n{steering_info}',
                 fontsize=13, fontweight='bold', pad=20)

        plt.grid(True, alpha=0.3)
        plt.legend(fontsize=10, loc='best')
        plt.axhline(y=0, color='black', linestyle=':', linewidth=1, alpha=0.5)

        # Add shaded region for "detects anomaly" (positive values)
        y_min, y_max = plt.ylim()
        if y_max > 0:
            plt.axhspan(0, y_max, alpha=0.05, color='green')
        if y_min < 0:
            plt.axhspan(y_min, 0, alpha=0.05, color='orange')

        # Add text box with questions
        textstr = f'Introspection: "{self.introspection_question}"\n\nControl: "{self.control_question}"'
        props = dict(boxstyle='round', facecolor='wheat', alpha=0.3)
        ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=9,
                verticalalignment='top', bbox=props, family='monospace')

        plt.tight_layout()

        # Save plot
        model_short = self.model_name.split("/")[-1].replace(".", "_")
        filename = f"introspection_{model_short}_contrastive_scale{magnitude}.png"
        plt.savefig(filename, dpi=150, bbox_inches='tight')
        print(f"\n[Plot saved to: {filename}]")
        plt.show()

    def _plot_scale_effects(
        self,
        scale_results: List[Dict],
        baseline_intro: float,
        baseline_control: float,
        layer_idx: int,
        contrastive_prompts: Tuple[str, str]
    ):
        """Plot logit difference vs steering vector scale for both questions.

        Args:
            scale_results: List of dicts with 'scale', 'intro_diff', 'control_diff' keys
            baseline_intro: Baseline introspection logit difference
            baseline_control: Baseline control logit difference
            layer_idx: Layer index where steering was applied
            contrastive_prompts: Tuple of (prompt1, prompt2) for contrastive vector
        """
        scales = [r['scale'] for r in scale_results]
        intro_diffs = [r['intro_diff'] for r in scale_results]
        control_diffs = [r['control_diff'] for r in scale_results]

        fig = plt.figure(figsize=(14, 9))
        ax = plt.subplot(111)

        # Plot baselines
        plt.axhline(y=baseline_intro, color='blue', linestyle='--', linewidth=1.5, alpha=0.5,
                   label=f'Baseline introspection: {baseline_intro:+.2f}')
        plt.axhline(y=baseline_control, color='red', linestyle='--', linewidth=1.5, alpha=0.5,
                   label=f'Baseline control: {baseline_control:+.2f}')

        # Plot introspection question (experimental)
        plt.plot(scales, intro_diffs, 'o-', linewidth=2.5, markersize=8, color='blue',
                label=f'Introspection question')

        # Plot control question (sanity check)
        plt.plot(scales, control_diffs, 's-', linewidth=2.5, markersize=8, color='red',
                label=f'Control question')

        # If multiple trials, show error bars
        if len(scale_results[0]['all_intro_diffs']) > 1:
            intro_stds = [np.std(r['all_intro_diffs']) for r in scale_results]
            control_stds = [np.std(r['all_control_diffs']) for r in scale_results]
            plt.errorbar(scales, intro_diffs, yerr=intro_stds, fmt='none', ecolor='blue',
                        capsize=5, alpha=0.4)
            plt.errorbar(scales, control_diffs, yerr=control_stds, fmt='none', ecolor='red',
                        capsize=5, alpha=0.4)

        # Styling
        plt.xlabel('Steering Vector Scale', fontsize=13)
        plt.ylabel('Logit(Yes) - Logit(No)', fontsize=13)

        # Build title with model and steering info
        model_display = self.model_name.split("/")[-1]
        steering_info = f'Contrastive Steering: "{contrastive_prompts[0]}" - "{contrastive_prompts[1]}" at layer {layer_idx}'

        plt.title(f'Steering vector scale sweep\n{model_display}\n{steering_info}',
                 fontsize=13, fontweight='bold', pad=20)

        plt.grid(True, alpha=0.3)
        plt.legend(fontsize=10, loc='best')
        plt.axhline(y=0, color='black', linestyle=':', linewidth=1, alpha=0.5)

        # Add shaded region for "detects anomaly" (positive values)
        y_min, y_max = plt.ylim()
        if y_max > 0:
            plt.axhspan(0, y_max, alpha=0.05, color='green')
        if y_min < 0:
            plt.axhspan(y_min, 0, alpha=0.05, color='orange')

        # Add text box with questions
        textstr = f'Introspection: "{self.introspection_question}"\n\nControl: "{self.control_question}"'
        props = dict(boxstyle='round', facecolor='wheat', alpha=0.3)
        ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=9,
                verticalalignment='top', bbox=props, family='monospace')

        plt.tight_layout()

        # Save plot
        model_short = self.model_name.split("/")[-1].replace(".", "_")
        filename = f"introspection_{model_short}_contrastive_layer{layer_idx}_scale_sweep.png"
        plt.savefig(filename, dpi=150, bbox_inches='tight')
        print(f"\n[Plot saved to: {filename}]")
        plt.show()

    def _plot_heatmaps(
        self,
        intro_matrix: np.ndarray,
        control_matrix: np.ndarray,
        layers: List[int],
        scales: List[float],
        baseline_intro: float,
        baseline_control: float,
        contrastive_prompts: Tuple[str, str]
    ):
        """Plot two heatmaps: introspection and control questions.

        Args:
            intro_matrix: Matrix of introspection logit differences (layers x scales)
            control_matrix: Matrix of control logit differences (layers x scales)
            layers: Layer indices
            scales: Scale values
            baseline_intro: Baseline introspection logit difference
            baseline_control: Baseline control logit difference
            contrastive_prompts: Tuple of (prompt1, prompt2) for title
        """
        model_display = self.model_name.split("/")[-1]
        model_short = model_display.replace(".", "_")

        # Transpose matrices: layers horizontal (x-axis), scales vertical (y-axis)
        intro_matrix_T = intro_matrix.T
        control_matrix_T = control_matrix.T

        # Create figure with two subplots side by side
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))

        # Determine common color scale limits for both plots
        vmin = min(intro_matrix.min(), control_matrix.min())
        vmax = max(intro_matrix.max(), control_matrix.max())

        # Use diverging colormap centered at 0
        norm = TwoSlopeNorm(vmin=vmin, vcenter=0, vmax=vmax)
        cmap = 'RdBu_r'  # Red for positive (Yes), Blue for negative (No)

        # Plot 1: Introspection question
        im1 = ax1.imshow(intro_matrix_T, aspect='auto', cmap=cmap, norm=norm,
                         extent=[layers[0], layers[-1], scales[0], scales[-1]],
                         origin='lower')
        ax1.set_xlabel('Layer Index', fontsize=12)
        ax1.set_ylabel('Steering Vector Scale', fontsize=12)
        ax1.set_title(f'Introspection Question\n"{self.introspection_question}"',
                     fontsize=11, fontweight='bold', pad=15)

        # Add baseline text
        ax1.text(0.02, 0.98, f'Baseline: {baseline_intro:+.2f}',
                transform=ax1.transAxes, fontsize=10,
                verticalalignment='top', bbox=dict(boxstyle='round',
                facecolor='wheat', alpha=0.5))

        # Plot 2: Control question
        im2 = ax2.imshow(control_matrix_T, aspect='auto', cmap=cmap, norm=norm,
                         extent=[layers[0], layers[-1], scales[0], scales[-1]],
                         origin='lower')
        ax2.set_xlabel('Layer Index', fontsize=12)
        ax2.set_ylabel('Steering Vector Scale', fontsize=12)
        ax2.set_title(f'Control Question\n"{self.control_question}"',
                     fontsize=11, fontweight='bold', pad=15)

        # Add baseline text
        ax2.text(0.02, 0.98, f'Baseline: {baseline_control:+.2f}',
                transform=ax2.transAxes, fontsize=10,
                verticalalignment='top', bbox=dict(boxstyle='round',
                facecolor='wheat', alpha=0.5))

        # Main title
        steering_info = f'Contrastive: "{contrastive_prompts[0]}" vs "{contrastive_prompts[1]}"'
        fig.suptitle(f'LLM Introspection Heatmap: {model_display}\n{steering_info}',
                    fontsize=13, fontweight='bold', y=0.98)

        # Add single shared colorbar
        fig.subplots_adjust(right=0.92)
        cbar_ax = fig.add_axes([0.94, 0.15, 0.02, 0.7])
        cbar = fig.colorbar(im2, cax=cbar_ax)
        cbar.set_label('Logit(Yes) - Logit(No)', fontsize=11)

        plt.tight_layout(rect=[0, 0, 0.92, 1])

        # Save plot
        filename = f"introspection_{model_short}_heatmap.png"
        plt.savefig(filename, dpi=150, bbox_inches='tight')
        print(f"\n[Heatmap saved to: {filename}]")
        plt.show()


def list_models():
    """Print available models organized by family."""
    print("\n=== Available Models ===\n")

    # Group by family
    families = {}
    for shortcut, config in MODEL_CONFIGS.items():
        family = config.get("family", "Other")
        if family not in families:
            families[family] = []
        families[family].append((shortcut, config))

    # Print Qwen family
    if "Qwen2.5" in families:
        print("Qwen2.5-Instruct Family (6 sizes):")
        for shortcut, config in sorted(families["Qwen2.5"], key=lambda x: x[1]["params"]):
            print(f"  {shortcut:20s} : {config['name']:50s} ({config['params']})")
        print()

    # Print Llama family
    if "Llama-3.x" in families:
        print("Llama 3.x Family (3 sizes):")
        for shortcut, config in sorted(families["Llama-3.x"], key=lambda x: x[1]["params"]):
            print(f"  {shortcut:20s} : {config['name']:50s} ({config['params']})")
        print()

    # Print Mistral family
    if "Mistral" in families:
        print("Mistral Family (1 size):")
        for shortcut, config in sorted(families["Mistral"], key=lambda x: x[1]["params"]):
            print(f"  {shortcut:20s} : {config['name']:50s} ({config['params']})")
        print()


def main():
    parser = argparse.ArgumentParser(
        description="LLM Introspection Experiment - Test emergence of introspection across model sizes",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Single model test
  python introspection.py --model qwen2.5-0.5b
  python introspection.py --model qwen2.5-7b --trials 3

  # Test with custom layers and magnitude
  python introspection.py --model qwen2.5-3b --layers 0 18 35 --scale 5

  # Generate heatmap sweeping over layers and scales
  python introspection.py --model qwen2.5-7b --heatmap
  python introspection.py --model qwen2.5-3b --heatmap --heatmap-scales 0 2 4 6 8 10

  # Generate actual text responses (temperature 0)
  python introspection.py --model qwen2.5-7b --generate --layer 14 --scale 8.0

  # List all available models
  python introspection.py --list-models
        """
    )
    parser.add_argument("--model", default="qwen2.5-0.5b",
                       help="Model name or shortcut (default: qwen2.5-0.5b)")
    parser.add_argument("--layers", nargs="+", type=int, default=None,
                       help="Layers to test (default: all layers)")
    parser.add_argument("--scale", type=float, default=1.0,
                       help="Scaling factor for steering vector (default: 1.0)")
    parser.add_argument("--trials", type=int, default=1,
                       help="Number of trials per condition (default: 1)")
    parser.add_argument("--token-pos", type=int, default=-1,
                       help="Token position to inject steering vector (-1 for last)")
    parser.add_argument("--contrastive", nargs=2, metavar=("PROMPT1", "PROMPT2"),
                       default=["Hi! How are you?", "HI! HOW ARE YOU?"],
                       help="Contrastive prompts to generate steering vector")
    parser.add_argument("--scale-sweep", action="store_true",
                       help="Run scale sweep experiment instead of layer sweep")
    parser.add_argument("--scales", nargs="+", type=float, default=None,
                       help="Scales to test in sweep")
    parser.add_argument("--sweep-layer", type=int, default=None,
                       help="Layer to use for scale sweep (default: middle layer)")
    parser.add_argument("--list-models", action="store_true",
                       help="List all available models and exit")
    parser.add_argument("--generate", action="store_true",
                       help="Generate actual text responses at temperature 0")
    parser.add_argument("--layer", type=int, default=None,
                       help="Layer index for --generate mode (default: middle layer)")
    parser.add_argument("--max-tokens", type=int, default=50,
                       help="Maximum tokens to generate in --generate mode (default: 50)")
    parser.add_argument("--heatmap", action="store_true",
                       help="Run heatmap sweep over layers and scales")
    parser.add_argument("--heatmap-layers", nargs="+", type=int, default=None,
                       help="Layers to include in heatmap (default: all layers)")
    parser.add_argument("--heatmap-scales", nargs="+", type=float, default=None,
                       help="Scales to include in heatmap (default: 0-10)")
    parser.add_argument("--verbose", action="store_true",
                       help="Enable verbose output (default: only show progress)")
    parser.add_argument("--steer-all-tokens", action="store_true",
                       help="Apply steering to all token positions")

    args = parser.parse_args()

    # List models if requested
    if args.list_models:
        list_models()
        return

    # Resolve model shortcut if used
    model_name = MODEL_CONFIGS.get(args.model, {}).get("name", args.model)

    print(f"Starting experiment with model: {model_name}\n")

    # Prepare contrastive prompts
    contrastive_prompts = tuple(args.contrastive)
    if args.verbose:
        print(f"Using contrastive prompts:")
        print(f"  Prompt 1: {repr(contrastive_prompts[0])}")
        print(f"  Prompt 2: {repr(contrastive_prompts[1])}\n")

    experiment = IntrospectionExperiment(model_name=model_name, verbose=args.verbose)

    # Choose experiment type
    if args.heatmap:
        print("Running heatmap sweep experiment\n")
        experiment.run_heatmap_sweep(
            layers=args.heatmap_layers,
            scales=args.heatmap_scales,
            token_pos=args.token_pos,
            contrastive_prompts=contrastive_prompts,
            steer_all_tokens=args.steer_all_tokens
        )
    elif args.generate:
        num_layers = len(experiment.layer_modules)
        layer = args.layer if args.layer is not None else num_layers // 2

        print(f"Running generation experiment at layer {layer}")
        print(f"Max tokens: {args.max_tokens}\n")

        experiment.run_generation_experiment(
            layer_idx=layer,
            magnitude=args.scale,
            token_pos=args.token_pos,
            contrastive_prompts=contrastive_prompts,
            max_new_tokens=args.max_tokens,
            steer_all_tokens=args.steer_all_tokens
        )
    elif args.scale_sweep:
        num_layers = len(experiment.layer_modules)
        sweep_layer = args.sweep_layer if args.sweep_layer is not None else num_layers // 2
        scales = args.scales if args.scales is not None else [0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 5.5, 6, 6.5, 7, 7.5, 8, 8.5, 9, 9.5, 10]

        print(f"Running scale sweep at layer {sweep_layer}")
        print(f"Scales to test: {scales}\n")

        experiment.run_scale_sweep(
            layer_idx=sweep_layer,
            scales=scales,
            num_trials=args.trials,
            token_pos=args.token_pos,
            contrastive_prompts=contrastive_prompts,
            plot=True,
            steer_all_tokens=args.steer_all_tokens
        )
    else:
        experiment.run_full_experiment(
            layers=args.layers,
            magnitude=args.scale,
            num_trials=args.trials,
            token_pos=args.token_pos,
            contrastive_prompts=contrastive_prompts,
            plot=True,
            steer_all_tokens=args.steer_all_tokens
        )


if __name__ == "__main__":
    main()
